{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1719359a",
   "metadata": {},
   "source": [
    "## Tools for NLP - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cc1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/venkat/Downloads/spark-3.2.0-bin-hadoop3.2')\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008eb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f66bf573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "399828ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "        (0,'Hi I am learning Spark'),\n",
    "        (1, 'I like spark'),\n",
    "        (2, 'Linear regression models are good')\n",
    "], ['label','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2b994a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|    0|Hi I am learning ...|\n",
      "|    1|        I like spark|\n",
      "|    2|Linear regression...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "051eece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2368b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = tokenizer.transform(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ac1dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------+---------------------------------------+\n",
      "|label|sentence                         |words                                  |\n",
      "+-----+---------------------------------+---------------------------------------+\n",
      "|0    |Hi I am learning Spark           |[hi, i, am, learning, spark]           |\n",
      "|1    |I like spark                     |[i, like, spark]                       |\n",
      "|2    |Linear regression models are good|[linear, regression, models, are, good]|\n",
      "+-----+---------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84c0b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_tf=HashingTF(inputCol='words', outputCol='rawFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0e0bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_data = hashing_tf.transform(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e5143db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+\n",
      "|rawFeatures                                                      |\n",
      "+-----------------------------------------------------------------+\n",
      "|(262144,[19036,33808,163984,173558,186845],[1.0,1.0,1.0,1.0,1.0])|\n",
      "|(262144,[19036,173558,208258],[1.0,1.0,1.0])                     |\n",
      "|(262144,[22249,46243,58267,113432,160975],[1.0,1.0,1.0,1.0,1.0]) |\n",
      "+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized_data.select('rawFeatures').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "815fbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol = 'rawFeatures', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f980dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_model = idf.fit(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d10d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4d7d35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/30 12:49:54 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "21/11/30 12:49:54 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                      |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |(262144,[19036,33808,163984,173558,186845],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453])|\n",
      "|1    |(262144,[19036,173558,208258],[0.28768207245178085,0.28768207245178085,0.6931471805599453])                                                   |\n",
      "|2    |(262144,[22249,46243,58267,113432,160975],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])   |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select('label','features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "369a3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect documents into vectors\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06376b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (0,\"a b c\".split(\" \")),\n",
    "        (1,\"a b b c a\".split(\" \"))\n",
    "],[\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bf3b43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef5e0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='words', outputCol='features', vocabSize=3, minDF = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d072ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fde9d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4976f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80928cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
